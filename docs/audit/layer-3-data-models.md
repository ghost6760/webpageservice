# Layer 3: Data Models Layer ‚Äî Technical Audit

**Layer**: Data Models & Persistence
**Analyzed**: 2025-11-07
**Files**: 5 Python modules
**Complexity**: High
**Status**: ‚úÖ Complete

---

## Executive Summary

The Data Models Layer provides the persistence and data management foundation for the application. It implements a **hybrid persistence strategy** using both **PostgreSQL (via SQLAlchemy ORM)** and **Redis (for caching/sessions)**. This layer includes sophisticated domain models for audit trails, conversations, and documents, with comprehensive multi-tenant isolation.

### Key Strengths
‚úÖ **Advanced audit trail system** ‚Äî Compensating transactions, immutable logs, comprehensive tracking (555 lines)
‚úÖ **Multi-tenant isolation** ‚Äî Redis prefix-based isolation, company-specific namespaces
‚úÖ **Hybrid persistence** ‚Äî PostgreSQL for structured data, Redis for cache/sessions
‚úÖ **Pydantic validation** ‚Äî Schema validation for API requests
‚úÖ **Clean domain models** ‚Äî Well-structured dataclasses with business logic
‚úÖ **Comprehensive document management** ‚Äî Full CRUD with vector store integration

### Critical Issues
üö® **No database migrations found** ‚Äî No Alembic/Flask-Migrate setup detected
üö® **Missing foreign key relationships** ‚Äî Models appear isolated (no ORM relationships defined)
üö® **No connection pooling config** ‚Äî SQLAlchemy connection pool not configured
üö® **Hardcoded Redis TTLs** ‚Äî Expiration times hardcoded throughout
üö® **No model tests** ‚Äî Zero unit tests for any model
üö® **JSON serialization in Redis** ‚Äî Inefficient storage of complex objects

### Maturity Score: **6.5/10**
- **Functionality**: 8/10 ‚Äî Comprehensive features, advanced audit system
- **Reliability**: 6/10 ‚Äî Works but lacks migrations and tests
- **Maintainability**: 7/10 ‚Äî Clean code but no documentation
- **Scalability**: 6/10 ‚Äî Redis-backed but no sharding strategy
- **Security**: 6/10 ‚Äî Multi-tenant isolation but no encryption

---

## 1. Structure & Inventory

### File Tree
```
app/models/
‚îú‚îÄ‚îÄ __init__.py                   # Package initialization (145 lines)
‚îú‚îÄ‚îÄ audit_trail.py                # Audit trail system (555 lines) ‚≠ê
‚îú‚îÄ‚îÄ conversation.py               # Conversation management (309 lines)
‚îú‚îÄ‚îÄ document.py                   # Document management (598 lines) ‚≠ê
‚îî‚îÄ‚îÄ schemas.py                    # Pydantic validation schemas (84 lines)
```

**Total Lines of Code**: ~1,691 lines
**Largest Models**:
1. `document.py` ‚Äî 598 lines
2. `audit_trail.py` ‚Äî 555 lines

---

## Component Breakdown

### **3.1 Audit Trail System** ‚≠ê (555 lines)
**File**: `app/models/audit_trail.py`
**Purpose**: Comprehensive audit trail with compensating transactions

#### Architecture
```python
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
from datetime import datetime
from enum import Enum
import json
import logging

logger = logging.getLogger(__name__)

class AuditAction(str, Enum):
    """Tipos de acciones auditables"""
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"
    READ = "read"
    EXECUTE = "execute"
    COMPENSATE = "compensate"
    ROLLBACK = "rollback"

class AuditResourceType(str, Enum):
    """Tipos de recursos auditables"""
    DOCUMENT = "document"
    CONVERSATION = "conversation"
    PROMPT = "prompt"
    WORKFLOW = "workflow"
    USER = "user"
    COMPANY_CONFIG = "company_config"
    OAUTH_CREDENTIAL = "oauth_credential"
```

**Analysis**:
- ‚úÖ Proper use of Enums for type safety
- ‚úÖ Comprehensive action and resource type coverage
- ‚úÖ Clean dataclass structure

#### Core Audit Entry Model
```python
@dataclass
class AuditEntry:
    """
    Registro de auditor√≠a inmutable.

    Attributes:
        audit_id: UUID √∫nico del registro
        timestamp: Momento exacto de la acci√≥n
        company_id: ID de la empresa (multi-tenant)
        user_id: ID del usuario que realiz√≥ la acci√≥n
        action: Tipo de acci√≥n (CREATE, UPDATE, DELETE, etc.)
        resource_type: Tipo de recurso afectado
        resource_id: ID del recurso espec√≠fico
        changes: Diccionario con cambios realizados (before/after)
        metadata: Informaci√≥n adicional contextual
        success: Si la acci√≥n fue exitosa
        error_message: Mensaje de error si fall√≥
        compensating_transaction_id: ID de transacci√≥n compensatoria si aplica
    """
    audit_id: str
    timestamp: str  # ISO format
    company_id: str
    user_id: str
    action: AuditAction
    resource_type: AuditResourceType
    resource_id: str
    changes: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    success: bool = True
    error_message: Optional[str] = None
    compensating_transaction_id: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convertir a diccionario para almacenamiento"""
        return {
            "audit_id": self.audit_id,
            "timestamp": self.timestamp,
            "company_id": self.company_id,
            "user_id": self.user_id,
            "action": self.action.value if isinstance(self.action, AuditAction) else self.action,
            "resource_type": self.resource_type.value if isinstance(self.resource_type, AuditResourceType) else self.resource_type,
            "resource_id": self.resource_id,
            "changes": self.changes,
            "metadata": self.metadata,
            "success": self.success,
            "error_message": self.error_message,
            "compensating_transaction_id": self.compensating_transaction_id
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AuditEntry':
        """Crear desde diccionario"""
        return cls(
            audit_id=data["audit_id"],
            timestamp=data["timestamp"],
            company_id=data["company_id"],
            user_id=data["user_id"],
            action=AuditAction(data["action"]) if isinstance(data["action"], str) else data["action"],
            resource_type=AuditResourceType(data["resource_type"]) if isinstance(data["resource_type"], str) else data["resource_type"],
            resource_id=data["resource_id"],
            changes=data.get("changes", {}),
            metadata=data.get("metadata", {}),
            success=data.get("success", True),
            error_message=data.get("error_message"),
            compensating_transaction_id=data.get("compensating_transaction_id")
        )
```

**Analysis**:
- ‚úÖ Immutable audit entries (dataclass with no setters)
- ‚úÖ Comprehensive metadata capture
- ‚úÖ Support for compensating transactions (SAGA pattern)
- ‚úÖ Serialization methods for persistence
- ‚ö†Ô∏è Timestamp as string (ISO format) instead of datetime object
- ‚ö†Ô∏è No validation of changes dictionary structure

#### Audit Manager
```python
class AuditManager:
    """
    Gestor de auditor√≠a con almacenamiento en PostgreSQL y Redis.

    Storage Strategy:
    - Hot logs (√∫ltimas 24h): Redis (r√°pido acceso)
    - Historical logs: PostgreSQL (b√∫squeda compleja, compliance)
    """

    def __init__(self, company_id: str, redis_client=None, db_session=None):
        self.company_id = company_id
        self.redis_client = redis_client
        self.db_session = db_session

        # Redis key patterns
        self.redis_prefix = f"{company_id}:audit:"
        self.hot_logs_key = f"{self.redis_prefix}hot_logs"
        self.index_key = f"{self.redis_prefix}index"

        # TTL para logs calientes (24 horas)
        self.hot_logs_ttl = 86400

    def log_action(
        self,
        user_id: str,
        action: AuditAction,
        resource_type: AuditResourceType,
        resource_id: str,
        changes: Dict[str, Any] = None,
        metadata: Dict[str, Any] = None,
        success: bool = True,
        error_message: Optional[str] = None
    ) -> AuditEntry:
        """
        Registrar una acci√≥n en el audit trail.

        Returns:
            AuditEntry creado y almacenado
        """
        import uuid

        audit_id = str(uuid.uuid4())
        timestamp = datetime.utcnow().isoformat()

        entry = AuditEntry(
            audit_id=audit_id,
            timestamp=timestamp,
            company_id=self.company_id,
            user_id=user_id,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            changes=changes or {},
            metadata=metadata or {},
            success=success,
            error_message=error_message
        )

        # Almacenar en Redis (hot logs)
        self._store_in_redis(entry)

        # Almacenar en PostgreSQL (historical)
        self._store_in_postgresql(entry)

        logger.info(f"[AUDIT] {action.value} on {resource_type.value}/{resource_id} by {user_id} (company: {self.company_id})")

        return entry
```

**Analysis**:
- ‚úÖ Dual storage strategy (Redis hot + PostgreSQL historical)
- ‚úÖ Automatic UUID generation
- ‚úÖ Comprehensive logging
- ‚ö†Ô∏è TTL hardcoded to 24 hours
- üö® No error handling for storage failures
- üö® No atomic guarantee between Redis and PostgreSQL writes

#### Compensating Transactions
```python
def create_compensating_transaction(
    self,
    original_audit_id: str,
    compensating_action: AuditAction,
    user_id: str,
    changes: Dict[str, Any] = None
) -> Optional[AuditEntry]:
    """
    Crear una transacci√≥n compensatoria para deshacer una acci√≥n previa.

    Implementa patr√≥n SAGA para transacciones distribuidas.

    Args:
        original_audit_id: ID de la auditor√≠a original a compensar
        compensating_action: Acci√≥n compensatoria (t√≠picamente ROLLBACK o COMPENSATE)
        user_id: Usuario que ejecuta la compensaci√≥n
        changes: Cambios realizados en la compensaci√≥n

    Returns:
        AuditEntry de la transacci√≥n compensatoria, o None si falla
    """
    # Obtener entrada original
    original_entry = self.get_audit_entry(original_audit_id)

    if not original_entry:
        logger.error(f"Original audit entry {original_audit_id} not found for compensation")
        return None

    # Crear entrada compensatoria
    compensating_entry = self.log_action(
        user_id=user_id,
        action=compensating_action,
        resource_type=original_entry.resource_type,
        resource_id=original_entry.resource_id,
        changes=changes or {"compensating_for": original_audit_id},
        metadata={
            "original_audit_id": original_audit_id,
            "original_action": original_entry.action.value,
            "compensation_reason": "Rollback or undo operation"
        }
    )

    # Actualizar entrada original con referencia a compensaci√≥n
    original_entry.compensating_transaction_id = compensating_entry.audit_id
    self._update_in_postgresql(original_entry)

    logger.info(f"[AUDIT] Compensating transaction created: {compensating_entry.audit_id} for {original_audit_id}")

    return compensating_entry
```

**Analysis**:
- ‚úÖ Proper SAGA pattern implementation
- ‚úÖ Bidirectional linking (original ‚Üî compensating)
- ‚úÖ Comprehensive metadata for compliance
- ‚ö†Ô∏è No transaction atomicity guarantees
- üö® PostgreSQL update can fail silently

#### Query Methods
```python
def get_audit_trail(
    self,
    resource_type: Optional[AuditResourceType] = None,
    resource_id: Optional[str] = None,
    user_id: Optional[str] = None,
    action: Optional[AuditAction] = None,
    start_time: Optional[str] = None,
    end_time: Optional[str] = None,
    limit: int = 100
) -> List[AuditEntry]:
    """
    Consultar audit trail con filtros.

    Strategy:
    1. Check Redis hot logs first (fast)
    2. Fall back to PostgreSQL for historical data
    """
    results = []

    # 1. Buscar en Redis (√∫ltimas 24h)
    hot_results = self._query_redis(
        resource_type=resource_type,
        resource_id=resource_id,
        user_id=user_id,
        action=action,
        limit=limit
    )
    results.extend(hot_results)

    # 2. Si no hay suficientes resultados o se busca data hist√≥rica, ir a PostgreSQL
    if len(results) < limit or (start_time and self._is_historical(start_time)):
        pg_results = self._query_postgresql(
            resource_type=resource_type,
            resource_id=resource_id,
            user_id=user_id,
            action=action,
            start_time=start_time,
            end_time=end_time,
            limit=limit - len(results)
        )
        results.extend(pg_results)

    # Ordenar por timestamp descendente
    results.sort(key=lambda x: x.timestamp, reverse=True)

    return results[:limit]
```

**Analysis**:
- ‚úÖ Intelligent two-tier query strategy
- ‚úÖ Results merge and deduplication
- ‚úÖ Proper sorting by timestamp
- ‚ö†Ô∏è No pagination support
- ‚ö†Ô∏è Potential performance issues with large result sets
- üö® No caching of PostgreSQL queries

#### Critical Issues

üö® **VULN-AT-001: No Atomic Dual Write**
```python
# Redis write
self._store_in_redis(entry)
# PostgreSQL write (can fail independently)
self._store_in_postgresql(entry)
```
If PostgreSQL write fails, Redis has inconsistent data.

**Fix**: Use 2-phase commit or eventual consistency with retry queue:
```python
try:
    self._store_in_redis(entry)
    self._store_in_postgresql(entry)
except Exception as e:
    # Rollback Redis or queue for retry
    self._enqueue_for_retry(entry)
    raise
```

üö® **VULN-AT-002: Hardcoded TTL**
```python
self.hot_logs_ttl = 86400  # 24 hours hardcoded
```

**Fix**: Make configurable per company:
```python
from app.config import get_company_config
config = get_company_config(company_id)
self.hot_logs_ttl = config.audit_log_ttl or 86400
```

---

### **3.2 Conversation Model** (309 lines)
**File**: `app/models/conversation.py`
**Purpose**: Conversation management with Redis-backed history (LangChain RedisChatMessageHistory)
**Updated**: 2025-11-12 (Critical fixes: token overflow & OOM prevention)

#### Architecture
```python
@dataclass
class ConversationMessage:
    """Mensaje individual en una conversaci√≥n"""
    role: str  # "user" or "assistant"
    content: str
    timestamp: str
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "role": self.role,
            "content": self.content,
            "timestamp": self.timestamp,
            "metadata": self.metadata
        }

class ConversationManager:
    """
    Gestor de conversaciones con almacenamiento en Redis.

    Features:
    - Historial de conversaci√≥n por usuario
    - Multi-tenant isolation
    - TTL autom√°tico
    - B√∫squeda de conversaciones
    """

    def __init__(self, company_id: str = "benova", redis_url: str = None):
        self.company_id = company_id

        if redis_url:
            self.redis_client = redis.from_url(redis_url, decode_responses=True)
        else:
            from app.services.redis_service import get_redis_client
            self.redis_client = get_redis_client()

        # Multi-tenant key prefix
        self.redis_prefix = f"{company_id}:conversation:"

        # TTL for conversations (7 days)
        self.conversation_ttl = 604800
```

**Analysis**:
- ‚úÖ Clean dataclass for messages
- ‚úÖ Multi-tenant key prefixing
- ‚úÖ Automatic TTL management
- ‚ö†Ô∏è Default company_id "benova" (should require explicit)
- üö® TTL hardcoded to 7 days
- üö® No pagination for long conversations

#### Core Methods
```python
def add_message(
    self,
    user_id: str,
    role: str,
    content: str,
    metadata: Dict[str, Any] = None
) -> ConversationMessage:
    """Agregar mensaje a la conversaci√≥n del usuario"""

    timestamp = datetime.utcnow().isoformat()
    message = ConversationMessage(
        role=role,
        content=content,
        timestamp=timestamp,
        metadata=metadata or {}
    )

    # Redis key para historial
    conversation_key = f"{self.redis_prefix}{user_id}"

    # Agregar mensaje a lista
    self.redis_client.rpush(conversation_key, json.dumps(message.to_dict()))

    # Renovar TTL
    self.redis_client.expire(conversation_key, self.conversation_ttl)

    logger.debug(f"[{self.company_id}] Message added for user {user_id}: {role}")

    return message

def get_conversation_history(
    self,
    user_id: str,
    limit: Optional[int] = None
) -> List[ConversationMessage]:
    """Obtener historial de conversaci√≥n"""

    conversation_key = f"{self.redis_prefix}{user_id}"

    # Obtener mensajes de Redis
    if limit:
        # √öltimos N mensajes
        messages_raw = self.redis_client.lrange(conversation_key, -limit, -1)
    else:
        # Todos los mensajes
        messages_raw = self.redis_client.lrange(conversation_key, 0, -1)

    # Parsear JSON
    messages = []
    for msg_json in messages_raw:
        try:
            msg_dict = json.loads(msg_json)
            messages.append(ConversationMessage(**msg_dict))
        except json.JSONDecodeError:
            logger.warning(f"Invalid message JSON in conversation: {msg_json}")
            continue

    return messages
```

**Analysis**:
- ‚úÖ Efficient Redis list operations
- ‚úÖ Automatic TTL renewal
- ‚úÖ Graceful JSON parsing error handling
- ‚úÖ **FIXED** (2025-11-12): Maximum conversation length enforced (`max_messages = 30`)
- ‚úÖ **FIXED** (2025-11-12): OOM prevention with Redis pre-truncation (BUG-CHAT-001)
- ‚ö†Ô∏è Note: `lrange` performance optimized via pre-truncation (loads max 30 messages)

#### Critical Issues & Fixes (2025-11-12)

üö® **VULN-CONV-001: Unbounded Conversation Growth** ‚Üí ‚úÖ **FIXED**

**Original Problem** (Pre-2025-11-12):
```python
# rpush without length check
self.redis_client.rpush(conversation_key, json.dumps(message.to_dict()))
```

Conversations could grow infinitely, causing:
- Memory issues (OOM kills)
- Token overflow errors (400K+ tokens)
- Production outages (Error 429)

**Fixes Implemented**:

**1. max_messages Limit** (Commit: `ac430be`, 2025-11-12)
```python
# app/models/conversation.py:41-43
def __init__(self, company_id: str = "benova", redis_url: str = None):
    # ...
    self.max_messages = 30  # Increased from 10 to 30
```

**2. _apply_message_window()** (Write-time enforcement)
```python
# app/models/conversation.py:85-107
def _apply_message_window(self, redis_history: RedisChatMessageHistory):
    """Apply sliding window limit to messages"""
    current_count = len(redis_history.messages)

    if current_count > self.max_messages:
        logger.warning(f"[{self.company_id}] Truncating history from {current_count} to {self.max_messages}")
        # Keep only last max_messages
        messages = redis_history.messages[-self.max_messages:]
        redis_history.clear()
        for msg in messages:
            redis_history.add_message(msg)
```

**3. get_chat_history() Truncation** (Commit: `957b48e`, 2025-11-12)

**Problem**: `get_chat_history()` returned ALL messages from Redis without enforcing limit.
**Impact**: Production error "Request too large: Limit 200000, Requested 400106"

**Fix** (Read-time enforcement):
```python
# app/models/conversation.py:63-74
def get_chat_history(self, user_id: str, format_type: str = "dict"):
    redis_history = self._get_or_create_redis_history(company_user_id)
    messages = redis_history.messages

    # ENFORCE max_messages limit at retrieval time
    if len(messages) > self.max_messages:
        logger.warning(f"Redis has {len(messages)} messages, truncating to {self.max_messages}")
        messages = messages[-self.max_messages:]
        # Clean up Redis to prevent future accumulation
        redis_history.clear()
        for msg in messages:
            redis_history.add_message(msg)

    return messages  # Returns TRUNCATED messages, not redis_history.messages
```

**4. OOM Prevention** (Commit: `3cd96f8`, BUG-CHAT-001, 2025-11-12)

**Problem**: Loading 65K messages into memory BEFORE truncating caused OOM kills.

**Root Cause Analysis**:
```python
# BEFORE (commit 957b48e - STILL HAD BUG):
messages = redis_history.messages  # ‚ùå Loads ALL 65,566 messages (~30MB)
if len(messages) > 30:
    messages = messages[-30:]       # ‚ùå Truncates AFTER loading
```

**Memory Impact**:
- Each request: 30MB loaded ‚Üí 150KB kept = 99.5% waste
- After 50 requests: 1.5GB+ memory usage ‚Üí OOM kill
- Worker killed: `[ERROR] Worker (pid:8) was sent SIGKILL! Perhaps out of memory?`

**Ultimate Fix** (Commit: `3cd96f8`):
```python
# app/models/conversation.py:64-83
def get_chat_history(self, user_id: str, format_type: str = "dict"):
    # ‚úÖ Step 1: Count messages WITHOUT loading (O(1), no memory)
    total_messages = self.redis_client.llen(history_key)

    # ‚úÖ Step 2: Truncate Redis BEFORE loading
    if total_messages > self.max_messages:
        logger.warning(
            f"[{self.company_id}] Redis has {total_messages} messages for {user_id}, "
            f"loading only last {self.max_messages} (max_messages limit)"
        )
        # Clean up Redis immediately to prevent future OOM
        self.redis_client.ltrim(history_key, -self.max_messages, -1)
        total_messages = self.max_messages

    # ‚úÖ Step 3: NOW load messages (guaranteed ‚â§ max_messages)
    redis_history = self._get_or_create_redis_history(company_user_id)
    messages = redis_history.messages  # Safe now, won't load more than 30
```

**Memory Improvement**:
- **Before**: 65,566 messages √ó 500 bytes = 32,783,000 bytes ‚âà **30 MB per request**
- **After**: 30 messages √ó 500 bytes = 15,000 bytes ‚âà **150 KB per request**
- **Reduction**: **99.5%** ‚úÖ

**Production Impact**:
- ‚ùå Workers dying after ~1 hour (OOM)
- ‚ùå Messages not processed (bot_inactive)
- ‚úÖ Workers stable indefinitely
- ‚úÖ Consistent memory usage

#### Defensive Layers (Triple Protection)

| Layer | When | Enforcement | Commit | Status |
|-------|------|-------------|--------|--------|
| **Write** | `add_message()` | `_apply_message_window()` | `ac430be` | ‚úÖ |
| **Read** | `get_chat_history()` | Direct truncation (Python) | `957b48e` | ‚úÖ (Fixed 444b434) |
| **Read** | `get_chat_history()` | Redis truncation (LTRIM) | `3cd96f8` | ‚úÖ (OOM fix) |
| **Summarization** | `memory_consolidation_node.py` | `MAX_MESSAGES_TO_SUMMARIZE = 50` | `50afa69` | ‚úÖ |

**Cross-References**:
- See `docs/audit/layer-7-ai-agents.md` ¬ß 13.16 for token overflow analysis
- See `docs/CHAT-HISTORY-ANALYSIS.md` ¬ß BUG-CHAT-001 for detailed OOM investigation

**Related Commits**:
- `ac430be` - Increase max_messages from 10 ‚Üí 30
- `50afa69` - Add MAX_MESSAGES_TO_SUMMARIZE safety layer
- `957b48e` - Enforce max_messages in get_chat_history()
- `444b434` - Fix return value bug (return truncated messages)
- `c19bcef` - Increase max_tokens to 2000 for fact extraction
- `3cd96f8` - **BUG-CHAT-001**: Prevent OOM by truncating Redis before loading

---

### **3.3 Document Model** ‚≠ê (598 lines)
**File**: `app/models/document.py`
**Purpose**: Document management with vector store integration

#### Architecture
```python
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
from datetime import datetime
import hashlib
import json
import redis
import logging

logger = logging.getLogger(__name__)

@dataclass
class Document:
    """
    Documento con metadata para almacenamiento y b√∫squeda.

    Attributes:
        doc_id: ID √∫nico del documento
        content: Contenido textual del documento
        metadata: Metadatos adicionales (title, source, etc.)
        embeddings: Vectores de embeddings (opcional)
        created_at: Timestamp de creaci√≥n
        updated_at: Timestamp de √∫ltima actualizaci√≥n
        company_id: ID de empresa (multi-tenant)
    """
    doc_id: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    embeddings: Optional[List[float]] = None
    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())
    updated_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())
    company_id: str = ""
```

**Analysis**:
- ‚úÖ Comprehensive document model
- ‚úÖ Optional embeddings for vector search
- ‚úÖ Timestamps for tracking
- ‚úÖ Multi-tenant company_id
- ‚ö†Ô∏è Empty default company_id (should validate)

#### Document Manager
```python
class DocumentManager:
    """
    Gestor de documentos con almacenamiento en Redis y vectorstore.

    Features:
    - CRUD completo de documentos
    - Integraci√≥n con vectorstore para embeddings
    - Multi-tenant isolation
    - Chunking autom√°tico de documentos grandes
    - Estad√≠sticas de uso
    """

    def __init__(self, company_id: str, redis_url: str = None):
        self.company_id = company_id

        if redis_url:
            self.redis_client = redis.from_url(redis_url, decode_responses=True)
        else:
            from app.services.redis_service import get_redis_client
            self.redis_client = get_redis_client()

        # Multi-tenant key prefix
        self.redis_prefix = f"{company_id}:document:"

        # Document metadata TTL (30 days)
        self.document_ttl = 2592000

    def add_document(
        self,
        content: str,
        metadata: Dict[str, Any],
        vectorstore_service = None
    ) -> tuple[str, int]:
        """
        Agregar documento al sistema.

        Process:
        1. Generar doc_id √∫nico
        2. Almacenar metadata en Redis
        3. Chunking del contenido
        4. Generar embeddings (v√≠a vectorstore_service)
        5. Almacenar chunks en vectorstore

        Returns:
            (doc_id, num_chunks)
        """
        import hashlib
        import time

        # Generar doc_id √∫nico
        doc_id = f"{self.company_id}_{hashlib.md5(content[:100].encode()).hexdigest()}_{int(time.time())}"

        # Asegurar company_id en metadata
        metadata['company_id'] = self.company_id
        metadata['doc_id'] = doc_id
        metadata['processed_at'] = datetime.utcnow().isoformat()

        # Almacenar metadata en Redis
        doc_key = f"{self.redis_prefix}{doc_id}"
        self.redis_client.hset(doc_key, mapping={
            "content": content,
            "metadata": json.dumps(metadata),
            "created_at": datetime.utcnow().isoformat()
        })
        self.redis_client.expire(doc_key, self.document_ttl)

        # Chunking y embeddings
        num_chunks = 0
        if vectorstore_service:
            # Delegar chunking al vectorstore
            chunks = vectorstore_service.add_documents([{
                "content": content,
                "metadata": metadata
            }])
            num_chunks = len(chunks) if chunks else 0

        logger.info(f"[{self.company_id}] Document {doc_id} added with {num_chunks} chunks")

        return doc_id, num_chunks
```

**Analysis**:
- ‚úÖ Unique ID generation with hash + timestamp
- ‚úÖ Automatic company_id injection
- ‚úÖ Delegation of chunking to vectorstore
- ‚úÖ TTL management
- ‚ö†Ô∏è Hash based on first 100 chars only (collision risk)
- üö® No error handling for Redis failures
- üö® No transaction atomicity

#### Bulk Operations
```python
def bulk_add_documents(
    self,
    documents: List[Dict[str, Any]],
    vectorstore_service = None
) -> Dict[str, Any]:
    """
    Agregar m√∫ltiples documentos en batch.

    Returns:
        {
            "total": int,
            "successful": int,
            "failed": int,
            "document_ids": List[str],
            "errors": List[str]
        }
    """
    results = {
        "total": len(documents),
        "successful": 0,
        "failed": 0,
        "document_ids": [],
        "errors": []
    }

    for doc_data in documents:
        try:
            content = doc_data.get("content", "")
            metadata = doc_data.get("metadata", {})

            if not content:
                results["failed"] += 1
                results["errors"].append("Empty content")
                continue

            doc_id, num_chunks = self.add_document(content, metadata, vectorstore_service)

            results["successful"] += 1
            results["document_ids"].append(doc_id)

        except Exception as e:
            results["failed"] += 1
            results["errors"].append(str(e))
            logger.error(f"Error adding document in bulk: {e}")

    return results
```

**Analysis**:
- ‚úÖ Proper error isolation per document
- ‚úÖ Comprehensive results summary
- ‚ö†Ô∏è No transaction rollback on partial failure
- ‚ö†Ô∏è Sequential processing (not batched for performance)

#### Critical Issues

üö® **VULN-DOC-001: Weak Document ID Generation**
```python
doc_id = f"{self.company_id}_{hashlib.md5(content[:100].encode()).hexdigest()}_{int(time.time())}"
```

MD5 hash of first 100 characters can collide, especially with templated documents.

**Fix**: Use UUID or full content hash:
```python
import uuid
doc_id = f"{self.company_id}_{uuid.uuid4()}"
```

üö® **VULN-DOC-002: No Atomic Add Operation**
```python
# Redis write
self.redis_client.hset(doc_key, ...)
# Vectorstore write (can fail)
chunks = vectorstore_service.add_documents(...)
```

If vectorstore fails, Redis has orphaned metadata.

**Fix**: Implement cleanup on failure:
```python
try:
    self.redis_client.hset(doc_key, ...)
    chunks = vectorstore_service.add_documents(...)
except Exception as e:
    # Cleanup Redis on failure
    self.redis_client.delete(doc_key)
    raise
```

---

### **3.4 Validation Schemas** (84 lines)
**File**: `app/models/schemas.py`
**Purpose**: Pydantic schemas for request/response validation

```python
from pydantic import BaseModel, Field, validator
from typing import Dict, Any, Optional, List
from datetime import datetime

class DocumentCreateSchema(BaseModel):
    """Schema para crear documento"""
    content: str = Field(..., min_length=1, description="Document content")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Document metadata")

    @validator('content')
    def content_not_empty(cls, v):
        if not v.strip():
            raise ValueError("Content cannot be empty or whitespace")
        return v

class ConversationMessageSchema(BaseModel):
    """Schema para mensaje de conversaci√≥n"""
    role: str = Field(..., regex="^(user|assistant)$")
    content: str = Field(..., min_length=1)
    metadata: Optional[Dict[str, Any]] = None

class AuditQuerySchema(BaseModel):
    """Schema para consultar audit trail"""
    resource_type: Optional[str] = None
    resource_id: Optional[str] = None
    user_id: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    limit: int = Field(default=100, ge=1, le=1000)
```

**Analysis**:
- ‚úÖ Proper Pydantic validation
- ‚úÖ Custom validators for business rules
- ‚úÖ Sensible default and limits
- ‚ö†Ô∏è Limited schema coverage (only 3 schemas for many models)

---

## 2. Integration Points

### Upstream Dependencies
- **Layer 1 (Config)**: `get_company_config()` for multi-tenant settings
- **Layer 2 (Services)**:
  - `RedisService` for Redis client
  - `VectorstoreService` for document embeddings
  - `OpenAIService` for embeddings generation

### Downstream Consumers
- **Layer 4 (Routes)**: All API endpoints consume models for CRUD operations
- **Layer 7 (Agents)**: Agents use `ConversationManager` for chat history

### External Dependencies
```python
# Data persistence
redis==5.x
psycopg2==2.9.x  # PostgreSQL driver
sqlalchemy==2.0.x  # ORM (if used)

# Validation
pydantic==2.x

# Utilities
python-dateutil==2.8.x
```

---

## 3. Functional Flow

### Document Ingestion Flow
```
1. User uploads document ‚Üí API Route (Layer 4)
2. Route validates with DocumentCreateSchema ‚Üí Layer 3
3. DocumentManager.add_document() called:
   ‚îú‚îÄ Generate unique doc_id
   ‚îú‚îÄ Store metadata in Redis
   ‚îú‚îÄ Chunk content via VectorstoreService (Layer 2)
   ‚îú‚îÄ Generate embeddings (OpenAI)
   ‚îî‚îÄ Store vectors in FAISS
4. Return doc_id + chunk_count to user
```

### Conversation Flow
```
1. User sends message ‚Üí API Route
2. ConversationManager.add_message(role="user", content=message)
   ‚îî‚îÄ Store in Redis list with TTL
3. Agent processes message (Layer 7)
4. ConversationManager.add_message(role="assistant", content=response)
5. ConversationManager.get_conversation_history(limit=10)
   ‚îî‚îÄ Retrieve last 10 messages for context
```

### Audit Trail Flow
```
1. Any sensitive operation (create/update/delete)
2. AuditManager.log_action() called:
   ‚îú‚îÄ Create AuditEntry with UUID
   ‚îú‚îÄ Store in Redis (hot logs, 24h TTL)
   ‚îî‚îÄ Store in PostgreSQL (historical, permanent)
3. Query with AuditManager.get_audit_trail():
   ‚îú‚îÄ Check Redis first (fast)
   ‚îî‚îÄ Fall back to PostgreSQL if historical
```

---

## 4. Bugs & Issues

### Critical Bugs (Already Documented)
- **VULN-AT-001**: No atomic dual write in audit system
- **VULN-AT-002**: Hardcoded TTLs throughout
- **VULN-CONV-001**: Unbounded conversation growth
- **VULN-DOC-001**: Weak document ID generation (MD5 collision risk)
- **VULN-DOC-002**: No atomic add operation

### Additional Issues

üö® **BUG-MOD-001: No Database Migrations**
**Severity**: Critical

**Issue**:
No Alembic or Flask-Migrate setup detected. Database schema changes require manual SQL.

**Impact**:
- Risky deployments
- Schema drift between environments
- No rollback capability

**Fix**:
```bash
# Initialize migrations
flask db init
flask db migrate -m "Initial schema"
flask db upgrade
```

üö® **BUG-MOD-002: No ORM Relationships**
**Severity**: High

**Issue**:
Models don't define SQLAlchemy relationships (e.g., Document ‚Üí AuditEntry).

**Evidence**:
```python
# No foreign keys or relationships defined
class Document:
    doc_id: str  # Should reference AuditEntry
```

**Fix**:
```python
class DocumentModel(Base):
    __tablename__ = 'documents'

    id = Column(String, primary_key=True)
    company_id = Column(String, ForeignKey('companies.id'))

    # Relationship
    audit_entries = relationship("AuditEntry", back_populates="document")
```

‚ö†Ô∏è **BUG-MOD-003: JSON Serialization in Redis**
**Severity**: Medium

**Issue**:
Complex objects stored as JSON strings in Redis (inefficient).

**Evidence**:
```python
self.redis_client.hset(doc_key, "metadata", json.dumps(metadata))
```

**Impact**:
- Slower serialization/deserialization
- Larger memory footprint
- Can't use Redis data structures (sets, sorted sets)

**Fix**: Use Redis hashes directly:
```python
# Instead of JSON string
self.redis_client.hset(doc_key, mapping=metadata)
```

---

## 5. Recommendations

### High Priority (Fix Immediately)

1. **Add Database Migrations**
   - Install Flask-Migrate/Alembic
   - Initialize migrations
   - Create initial schema migration
   - Add to deployment pipeline

2. **Define ORM Relationships**
   - Convert dataclasses to SQLAlchemy models
   - Add foreign keys between models
   - Implement cascade delete rules

3. **Fix Atomic Operations**
   - Add 2-phase commit for dual writes
   - Implement cleanup on failures
   - Add retry queues for failed writes

4. **Add Model Tests**
   - Unit tests for each model
   - Integration tests for persistence
   - Test multi-tenant isolation

### Medium Priority (Next Sprint)

5. **Implement Connection Pooling**
   ```python
   from sqlalchemy import create_engine
   from sqlalchemy.pool import QueuePool

   engine = create_engine(
       DATABASE_URL,
       poolclass=QueuePool,
       pool_size=10,
       max_overflow=20
   )
   ```

6. **Add Pagination Support**
   - Implement cursor-based pagination
   - Add page size limits
   - Return total count metadata

7. **Replace MD5 with UUID**
   - Use `uuid.uuid4()` for document IDs
   - Migrate existing document IDs

8. **Make TTLs Configurable**
   - Move to company config
   - Allow per-resource-type TTLs

### Low Priority (Technical Debt)

9. **Add Redis Compression**
   - Compress large JSON values
   - Use MessagePack instead of JSON

10. **Implement Model Versioning**
    - Add version field to all models
    - Track schema migrations

11. **Add Soft Delete**
    - Add `deleted_at` field
    - Filter deleted records in queries

12. **Add Full-Text Search**
    - PostgreSQL full-text search for documents
    - ElasticSearch integration option

---

## 6. Suggested PRs/Issues

| ID | Title | Priority | Effort |
|----|-------|----------|--------|
| **PR-MOD-001** | Add Flask-Migrate for database migrations | High | 4h |
| **PR-MOD-002** | Convert dataclasses to SQLAlchemy ORM models | High | 8h |
| **PR-MOD-003** | Fix atomic operations in dual writes | High | 6h |
| **PR-MOD-004** | Add comprehensive model unit tests | High | 12h |
| **PR-MOD-005** | Implement connection pooling for PostgreSQL | Medium | 3h |
| **PR-MOD-006** | Add pagination to all list methods | Medium | 5h |
| **PR-MOD-007** | Replace MD5 with UUID for document IDs | Medium | 4h |
| **PR-MOD-008** | Make TTLs configurable per company | Medium | 3h |
| **PR-MOD-009** | Add Redis compression for large values | Low | 4h |
| **PR-MOD-010** | Implement soft delete across all models | Low | 6h |

---

## 7. Dependencies

### Python Packages
```python
# ORM & Database
sqlalchemy==2.0.x
psycopg2-binary==2.9.x
alembic==1.12.x  # MISSING - needs to be added

# Caching
redis==5.x

# Validation
pydantic==2.x

# Utilities
python-dateutil==2.8.x
```

### Missing Dependencies
- ‚ùå No Alembic (database migrations)
- ‚ùå No pytest fixtures for model testing

---

## 8. Maturity & Quality Metrics

| Metric | Score | Notes |
|--------|-------|-------|
| **Functionality** | 8/10 | Comprehensive feature set |
| **Reliability** | 6/10 | Works but lacks migrations/tests |
| **Maintainability** | 7/10 | Clean code, needs docs |
| **Testability** | 3/10 | Zero tests |
| **Scalability** | 6/10 | Redis-backed, no sharding |
| **Security** | 6/10 | Multi-tenant isolation OK |
| **Observability** | 5/10 | Basic logging only |

**Overall: 6.5/10**

---

## 9. Developer Guidance

### Adding a New Model

1. Create dataclass in `app/models/`:
```python
from dataclasses import dataclass, field
from datetime import datetime

@dataclass
class MyModel:
    id: str
    company_id: str
    data: Dict[str, Any] = field(default_factory=dict)
    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())
```

2. Add Pydantic schema in `schemas.py`:
```python
class MyModelSchema(BaseModel):
    data: Dict[str, Any]

    @validator('data')
    def validate_data(cls, v):
        # Custom validation
        return v
```

3. Create manager class for persistence:
```python
class MyModelManager:
    def __init__(self, company_id: str):
        self.company_id = company_id
        # Initialize Redis/PostgreSQL clients
```

4. Write tests:
```python
def test_my_model_creation():
    model = MyModel(id="test", company_id="benova")
    assert model.company_id == "benova"
```

### Multi-Tenant Best Practices

‚úÖ **Always prefix Redis keys**:
```python
key = f"{company_id}:resource:{resource_id}"
```

‚úÖ **Always include company_id in metadata**:
```python
metadata['company_id'] = self.company_id
```

‚úÖ **Validate company_id before operations**:
```python
from app.config import get_company_config
if not get_company_config(company_id):
    raise ValueError(f"Invalid company: {company_id}")
```

### Common Pitfalls

‚ùå **Don't forget TTL**:
```python
# BAD
redis_client.set(key, value)

# GOOD
redis_client.setex(key, ttl, value)
```

‚ùå **Don't skip error handling**:
```python
# BAD
self._store_in_redis(entry)
self._store_in_postgresql(entry)

# GOOD
try:
    self._store_in_redis(entry)
    self._store_in_postgresql(entry)
except Exception as e:
    logger.error(f"Storage failed: {e}")
    # Cleanup or retry
```

---

## 10. Conclusion

The Data Models Layer provides a **solid foundation** with sophisticated audit trails and multi-tenant isolation, but has **critical gaps** in database management and testing.

### Strengths
‚úÖ Advanced audit trail with compensating transactions
‚úÖ Clean domain models with comprehensive features
‚úÖ Multi-tenant isolation throughout
‚úÖ Hybrid Redis/PostgreSQL persistence strategy

### Critical Gaps
üö® No database migrations (manual schema changes)
üö® No ORM relationships (isolated models)
üö® Zero model tests
üö® No atomic operations for dual writes
üö® Hardcoded configurations throughout

### Immediate Action Items
1. Add Flask-Migrate for database migrations (4 hours)
2. Convert to SQLAlchemy ORM models with relationships (8 hours)
3. Add comprehensive model tests (12 hours)
4. Fix atomic operations (6 hours)

**Estimated Remediation**: 30-35 hours for high-priority fixes.

---

**Previous**: [‚Üê Layer 2: Core Services](layer-2-core-services.md)
**Next**: [Layer 4: API Routes ‚Üí](layer-4-api-routes.md)
